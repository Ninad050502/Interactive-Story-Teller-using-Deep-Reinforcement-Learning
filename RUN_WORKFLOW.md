# Complete Workflow Guide

## ðŸŽ¯ Recommended Workflow

### Step 1: Train and Evaluate Model

Run the complete training and evaluation pipeline:

```bash
cd src
python train_and_evaluate.py
```

**What this does:**
1. âœ… Trains DQN agent on training split
2. âœ… Saves model to `models/saved_dqn.pt`
3. âœ… Evaluates on train split (no exploration)
4. âœ… Evaluates on dev split
5. âœ… Evaluates on test split
6. âœ… Shows comparison across all splits

**Expected Output:**
```
============================================================
ðŸš€ DQN Training and Evaluation Pipeline
============================================================

ðŸ“š STEP 1: Training the Model
============================================================
[Training progress...]
âœ… Training complete. Model saved to models/saved_dqn.pt

ðŸ“Š STEP 1.5: Evaluating on TRAIN Split
============================================================
ðŸ“Š TRAIN Split Evaluation Summary
  Average Reward: 4.85 Â± 0.45
  ...

ðŸ“Š STEP 2: Evaluating on DEV Split
============================================================
ðŸ“Š DEV Split Evaluation Summary
  Average Reward: 4.75 Â± 0.45
  ...

ðŸ“Š STEP 3: Evaluating on TEST Split
============================================================
ðŸ“Š TEST Split Evaluation Summary
  Average Reward: 4.68 Â± 0.52
  ...

ðŸ“Š FINAL RESULTS: Train vs Dev vs Test Comparison
============================================================
  Train - Average Reward: 4.85 Â± 0.45
  Dev   - Average Reward: 4.75 Â± 0.45
  Test  - Average Reward: 4.68 Â± 0.52
  âœ… Good generalization
```

**Time:** ~30-60 minutes (depending on episodes and dataset size)

---

### Step 2: Compare with Baselines

After training, compare your DQN agent against baselines:

```bash
cd src
python baseline_comparison.py --episodes 100
```

**Options:**
- `--episodes N`: Number of evaluation episodes (default: 100)
- `--generation`: Use generation mode (if you trained with generation)
- `--stochastic-emotions`: Use stochastic emotions (default: True)

**Example with generation mode:**
```bash
python baseline_comparison.py --episodes 100 --generation
```

**Expected Output:**
```
============================================================
Baseline Comparison: Random vs Oracle vs DQN Agent
============================================================

ðŸŽ² Evaluating Random Baseline...
âœ… Random baseline complete

ðŸ”® Evaluating Oracle Baseline...
âœ… Oracle baseline complete

ðŸ¤– Evaluating DQN Agent...
âœ… Loaded model from models/saved_dqn.pt
âœ… DQN agent evaluation complete

============================================================
ðŸ“Š BASELINE COMPARISON RESULTS
============================================================

Random:
  Average Reward: 2.34 Â± 0.45
  True Continuation Pick Rate: 33.3%

Oracle:
  Average Reward: 5.23 Â± 0.12
  True Continuation Pick Rate: 100.0%

DQN Agent:
  Average Reward: 4.87 Â± 0.38
  True Continuation Pick Rate: 78.5%
  Avg Ending Reward: 3.2

ðŸ“ˆ DQN Improvement over Random: +2.53 (+108.1%)
ðŸ“‰ Gap to Oracle: -0.36 (-6.9%)
```

**Time:** ~5-10 minutes (depending on episodes)

---

## ðŸ”§ Configuration Before Running

### Option A: Default Settings (Recommended for First Run)

Edit `config.py`:
```python
USE_GENERATION = False  # Start without generation (faster)
USE_STOCHASTIC_EMOTIONS = True  # Enable stochastic emotions
INCLUDE_SCENE_INDEX = True  # Include scene index
```

### Option B: Full Features (Slower but Complete)

Edit `config.py`:
```python
USE_GENERATION = True  # Enable generation mode
USE_STOCHASTIC_EMOTIONS = True  # Enable stochastic emotions
INCLUDE_SCENE_INDEX = True  # Include scene index
```

---

## ðŸ“Š What You'll Get

### From train_and_evaluate.py:
- âœ… Trained model saved to `models/saved_dqn.pt`
- âœ… Performance metrics on train/dev/test splits
- âœ… Overfitting detection
- âœ… Generalization analysis

### From baseline_comparison.py:
- âœ… Random baseline performance
- âœ… Oracle baseline performance
- âœ… DQN agent performance
- âœ… Improvement metrics
- âœ… True continuation pick rate
- âœ… Ending quality metrics

---

## âš¡ Quick Test (Faster)

For a quick test with limited stories:

```python
# In train_and_evaluate.py, modify:
TRAIN_MAX_STORIES = 50  # Limit training stories
EVAL_MAX_STORIES = 20   # Limit evaluation stories
```

Then run:
```bash
cd src
python train_and_evaluate.py
python baseline_comparison.py --episodes 20
```

---

## ðŸŽ“ Understanding the Results

### Good Results:
- **DQN > Random**: Agent learned something
- **DQN close to Oracle**: Agent learned well
- **Small dev-test gap**: Good generalization
- **High true pick rate**: Agent prefers good continuations

### Areas for Improvement:
- **Large train-dev gap**: Possible overfitting
- **DQN â‰ˆ Random**: Agent didn't learn (check hyperparameters)
- **Large dev-test gap**: Poor generalization

---

## âœ… Complete Workflow Summary

1. **Configure** `config.py` (optional - defaults work)
2. **Train & Evaluate**: `python train_and_evaluate.py`
3. **Compare Baselines**: `python baseline_comparison.py --episodes 100`

That's it! You'll have:
- âœ… Trained model
- âœ… Performance metrics
- âœ… Baseline comparisons
- âœ… All proposal requirements met

